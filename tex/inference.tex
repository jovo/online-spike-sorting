\newcommand{\tx}{\tilde{x}}

%       \begin{algorithm}
%       \caption{State-dependent thinning for sMJPs}\label{alg:smjp_unif}
%       \begin{tabular}{p{1.2cm}p{12.4cm}}
%       Input:&  Hazard functions $A_{ss'}(\cdot)\ \forall s,s' \in \mathcal{S}$, and an initial distribution over states $\pi_0$. \\ 
%        & Dominating hazard functions $B_{s}(\tau) \ge A_{s}(\tau)\ \forall \tau, s$, where $A_{s}(\tau) = \sum_{s'} A_{ss'} (\tau)$. \\
%       Output:& A piecewise constant path $(V,L,W) \equiv (v_i, l_i, w_i)$ on the interval $[t_{start},t_{end}]$. % defined by a set of state and time pairs.
%       \end{tabular}
%       \begin{algorithmic}[1]
%       \State Draw $v_0 \sim \pi_0$ and set $w_0 = t_{start}$. Set $l_0 = 0$ and $i = 0$.
%       %\State Set $A_{s_i}(\tau) = \sum_j A_{s_i,j} (\tau)$ and define $u_{s_i}(\tau) \ge A_{s_i}(\tau) \forall \tau$. \label{alg:loop}
%       %\State Let $\tau_o = (w_{i} - l_i)$. \label{alg:smjp_loop}
%       \While{$w_i < t_{end}$}
%       \State Sample $\tau_{hold} \sim B_{v_i}(\cdot), \text{\ with } \tau_{hold} > l_i$.
%              Let $\Delta w_i = \tau_{hold} - l_i$, and $w_{i+1} = w_i + \Delta w_i$. 
%       %\State Sample $\tau_n \sim B_{v_i}(\tau_n) \exp \left( -\int_{\tau_o}^{\tau_n} B_{v_i}(\tau) d\tau \right), \text{\ conditioned on } \tau_n > \tau_o$.
%       %       Let $w_{i+1} = \tau_n + l_i$. 
%       \If{$\frac{A_{v_i}(\tau_{hold})}{B_{v_i}(\tau_{hold})}$}   \label{enum:thin}
%         \State Set $l_{i+1} = 0$, and sample $v_{i+1}$, with $P(v_{i+1} = s'|v_i) \propto A_{v_is'}(\tau_{hold}),\ s' \in \mathcal{S}$.   \label{enum:sample}
%       \Else
%         \State Set $l_{i+1} = l_i + \Delta w_i$, and $v_{i+1} = v_i$.
%       \EndIf
%       %\State Define $q_j = \frac{A_{v_ss'}(\tau_n)}{B_{v_i}(\tau_n)}\ \forall j \in \mathcal{S}$ and $q_0 = 1 - \sum_{j \in  \cS} q_j$.% = 1 - \frac{A_{v_i}(\tau)}{u_{v_i}(\tau)}$.
%       %\State Sample $v_{i+1} \sim q$. If $v_{i+1} \neq 0$, set $l_{i+1} = w_{i+1}$; else set $v_{i+1}$ to $v_i$ and $l_{i+1} = l_i$.
%       \State Increment $i$.
%       \EndWhile
%       \State Set $w_{|W|} = t_{end}$, $v_{|W|} = v_{|W|-1}$, $l_{|W|} = l_{|W|} + w_{|W|} - w_{|W|-1}$. 
%       \end{algorithmic}
%       \end{algorithm}


We now address the problem of posterior inference over the latent variables gives the matrix $\bX$ of multielectrode recordings. Unsurprisingly, exact 
inference is intractable, and we have to resort to approximating the posterior distribution.
There exists a vast literature on approximate inference for Bayesian nonparametric models, especially so for models based on the Dirichlet process.
Traditional approaches are sampling-based, typically involving Markov chain Monte Carlo techniques (see eg.\ \citep{Nea2000, IshJam2001}), 
and recently there has also been work on constructing deterministic approximations to the intractable posterior (eg.\ \citep{BleJor2006, MinGha2003}).
Our problem is complicated by two factors. The first is the convolutional nature of our observation process, where we observe a functional of the 
sequence of observations drawn from the DP mixture model. This is in contrast to the usual situation where one directly observes the DPMM outputs themselves.
The second complication is a computational requirement: typical inference schemes are batch methods that are slow and computationally expensive. 
Our ultimate goal, on the other hand, is to perform inference in real time, so that these batch approaches are unsuitable for our purposes.

With the latter objective in mind, we develop an online algorithm for posterior inference. Our algorithm is based on \cite{WangDun2009}, though this was 
developed for the usual case of i.i.d.\ observations drawn from a DPMM. We generalize this algorithm to our observation process, and also 
to account for the time-evolution of the cluster parameters and the Markov nature of the observation noise.

At a high level, at time $t$, our algorithm maintains the set of times of the spikes it has inferred from the observations until time $t$. It also maintains
the identities of the neurons that it assigned each of these spikes to, as well as the weight vectors determining the shapes of the associated spike 
waveforms. In addition to these point estimates, the algorithm also maintains a set of posterior distributions $q_{it}(\theta^*_i)$ where $i$ spans over the
set of neurons associated with the spikes seen so far. Let the number of unique neurons observed at time $t$ be $C_t$, so that $i$ varies from $1$ to $C_t$.
For each $i$, $q_{it}(\theta^*_i)$ approximates the distribution over the parameters 
$\theta_i^* \equiv (\mu_i^*, \Sigma_i^*)$ of neuron $i$ given the observations until time $t$. 
Having identified the location and shape of spikes from earlier times, we can calculate their contribution to the recording $x_{t+1}$ at time $t+1$.
We subtract this term from $x_{t+1}$, and treat the residual $\tx_{t+1}$ as an observation from a DP mixture model.
Given this residual, our algorithm then makes a hard decision about whether or not this was produced by an underlying spike, what neuron that spike belongs 
to (one of the earlier neurons or a new neuron), and what the shape of the associated spike waveform is. The latter is used to calculate
$q_{i,t+1}(\theta^*_i)$, the new distribution over neuron parameters at time $t+1$. Our algorithm proceeds recursively in this manner. 
We describe it in more detail below.


Recall that each spike waveform spans $L$ time bins. 
Let $z_t$ indicate whether or not a spike is present at time $t$, with $\by_t$ giving its shape, and $\nu_t$ is associated neuron. 
The residual at time $t$ is then given by
\begin{align}
  \tx_t = x_t - \sum_{i=1}^L \bA\by_{t-i} \label{eq:resid}
\end{align}
Given this residual, the first step is to decide whether or not there is a spike underlying this residual.
By Bayes' rule,
\begin{align}
  P(z_t = 1 | \tx_t)  &\propto P(z_t = 1, \tx_t) = \sum_{\nu_t = 1}^{C_{t-1}+1} P(\tx_t, {\nu_t} | z_t = 1) P(z_t = 1) \\
\intertext{Here, $C_{t-1}$ is the number of unique neurons underlying the observations before time $t$. Letting $n_i$ be the number of spikes from neuron 
$i$, we have from the Chinese restaurant process that}
  P({\nu_t} = i | z_t = 1) & \propto 
  \begin{cases}
   n_i \quad i \in \{1,\cdots, C_{t-1}\} \\
   \alpha \quad\ i = C_{t-1} + 1 \\
  \end{cases}  \label{eq:crp_marg}\\
\intertext{On the other hand,}
  P(\tx_t | {\nu_t = i} , z_t = 1) &= \int_{\Theta} P(\tx_t | \theta_t) q_{it} (\theta_t) \dd \theta_t  \label{eq:norm_nw}
\end{align}
Recall that  $P(\tx_t | \theta_t)$ is just the normal distribution with mean $\mu_t$ and variance $\theta_t$. We let $q_{it}(\cdot)$ be the normal-Wishart
distribution. % with parameters 
We can then evaluate integral \eqref{eq:norm_nw}, and then summation \eqref{eq:crp_marg} to calculate $P(z_t = 1 | \tx_t)$. 
If this exceeds a threshold of $0.5$ we decide that there is a spike present at time $t$, otherwise, we set $z_t = 0$.
Note that we make this decision after marginalizing over all possible cluster assignments and all values of the weight vector $\by_t$.
In the event that we decide that a spike is present , we simplify matters collapsing these parameters, and the spike the neuron and weight-vector
with highest posterior probability. Given these two, we update the posterior distribution over parameters of cluster $\nu_t$, thereby obtaining
$q_{i,t+1}$.
