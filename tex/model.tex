\newcommand{\by}{\mathbf{y}}
\newcommand{\bA}{\mathbf{A}}
%\nipsfinalcopy % Uncomment for camera-ready version

\section{The model/introduction}
Our data is a time-series of multielectrode recordings $\bX \equiv (\bx_1, \cdots, \bx_T)$, and consists of $T$ recordings from $C$ channels. 
The set of recording times lie on regular grid with interval length $\Delta$, while $\bx_t \in \mathbb{R}^C$ for all $t$. 
This time-series of electrical activity is driven by an unknown number of neurons {\color{red} and we want to... recap scientific goals}. 
We let the number of neurons be unbounded, though only a few of the infinite
neurons dominate. These neurons contribute the majority of the activity in any finite interval of time; however, as time passes, the total number of 
observed neurons increases {\color{red}(Justify?)}. 
%Each neuron, has its own `shape' A natural model in such a situation is to
The neurons themselves generate continuous-time voltage traces, with the outputs of all neurons superimposed and discretely sampled to produce the 
recordings $\bX$.  At a high level, we model the output of each neuron as a
series of idealized spikes which are smoothed with appropriate kernels, the latter determining the shape of each spike. 
%Each neuron has its own distribution over waveform shapes. 
We describe this in detail, starting first with a model for a single channel recording $X \equiv (x_1, \cdots, x_T)$.

\subsection{Modelling a single electrode recording}
There is a rich literature characterizing the spiking activity of a single neuron \citep{}, accounting in detail for factors like nonstationarity, 
refractoriness and spike waveform. We shall however make a number of simplifying assumptions. First, we model the spiking activity of each neuron is 
stationary and memoryless, so that its set of spike times are 
distributed as a homogeneous Poisson process. 
{\color{red} justify?} We model the neurons themselves are heterogeneous, with the $i$th neuron having
an (unknown) firing rate $r_i$. Call the ordered set of spike times of the $i$th neuron $E_i$; then the time between successive elements of $E_i$ is 
exponentially distributed with mean $1/r_i$. We write this as
\begin{align}
  E_i &\sim \text{PoissProc}(r_i)
\end{align}
The actual electrical output of a neuron is not binary; instead each spiking event is a smooth perturbation in voltage about a
resting state. This perturbation forms the shape of the spike, and without any loss of generality, we set the resting state to zero. 
{(\color{red} figure? better biological description? comment on how we preprocess the data to get zero mean?)}. 
While the spike shapes vary across neurons as well as across different spikes of the same neuron, each 
neuron has its own characteristic distribution over shapes. {\color{red} Figure? } 
We let $\theta^* \in \Theta$ parametrize this distribution, with neuron $i$ having parameter $\theta^*_i$. Whenever this neuron emits a 
spike, a new shape is drawn independently from the corresponding distribution. %$p_{\theta_i}$, and 
This waveform is then offset to the time of the spike, and forms the voltage trace associate with that spike. The complete output of the neuron is the 
superposition of all these spike waveforms. 
{\color{red} ( Figure?)} % Comment on how this dictionary is obtained now, or in section on inference?)}. 
We model the random spike shapes themselves as weighted superpositions of a dictionary of $K$ basis functions $A \equiv (A_1(t), \cdots, A_K(t))$. The
dictionary elements are shared across all neurons, and each is a real-valued function of time.
For the $i$th neuron, the $j$th spike $e_{ij} \in E_i$, is associated with a random $K$-dimensional weight vector $\tilde{\by}_{ij}$, and the 
shape of this spike is given by the weighted sum $\sum_{k=1}^K \tilde{y}_{ijk} A_k(t)$. We let $\tilde{\by}_{ij}$ be Gaussian distributed, with 
$\theta^*_i \equiv (\mu^*_i, \Sigma^*_i)$ determining its 
mean and variance. Then, at any time $t$, the output of neuron $i$ is
\begin{align}
  x_{i}(t) &= \sum_{j=1}^{|E_i|} \sum_{k=1}^K \tilde{y}_{ijk} A_k(t - e_{ij})
\end{align}

{The total signal recorded $x(t)$ at any electrode is the superposition of the outputs of all neurons. Assume for the moment there are $N$
neurons, and define $E = \cup_{i=1}^{N} E_i$ as
the (ordered) union of the spike times of all neurons. 
To map elements $e_j \in E$ to those in the individual spike trains $E_i$, we need to introduce some more notation. 
Let $\nu_j$ be the neuron to which the $j$th element of $E$ belongs, and let $\theta_j \equiv (\mu_j, \Sigma_j)$ be the neuron parameter associated with
that spike. Thus $\theta_j = \theta^*_{\nu_j}$. Furthermore, let $p_j$ index the position of the spike $j$ of $E$ in $E_{\nu_j}$, the spike train
of neuron $\nu_j$. Thus $e_j = e_{\nu_j p_j}$. Finally, define $\by_{j} = \tilde{\by}_{\nu_j p_j}$ as the weight vector of spike $e_j$. Then, we have that}
\begin{align}
  x(t) &= \sum_{i=1}^{N} x_{i}(t) =   \sum_{j=1}^{|E|} \sum_{k=1}^K y_{jk} A_k(t - e_{j}) \label{eq:spk_sup}
\intertext{where}
  \by_{j} & \sim N(\mu_{j}, \Sigma_{j}) \label{eq:spk_shape}
\end{align}

From the superposition property of the Poisson process \citep{kingman93}, the overall spiking activity $E$ is a 
Poisson process with rate $R = \sum_{i=1}^{N} r_i$. Each event $e_j \in E$ is associated with a pair of labels, the parameter of the neuron to which it 
is assigned ($\theta_j = (\mu_j, \Sigma_j)$), and the weight-vector characterizing the spike shape ($\by_j$). We can view these as the `marks' of a 
marked Poisson process $E$.  From the properties of the Poisson process, we have that the marks $\theta_j$ are drawn i.i.d. from a probability measure 
\begin{align}
 G(\dd \theta) = \frac{1}{R}\sum_{i=1}^{N} r_i \delta_{\theta^*_i}    \label{eq:mark_distr}
\end{align}
Note that with probability one, the neurons have distinct parameters, so that the mark $\theta_j$ associated with spike $j$ identifies the
neuron which produced it: $G(\theta_j = \theta^*_i) = P(\nu_j = i) = \frac{r_i}{R}$. Given $\theta_j$, $y_j$ is distributed as in
equation \ref{eq:spk_shape}. The output waveform $x(t)$ is then a linear functional of this marked Poisson process (equation \eqref{eq:spk_sup})
{\color{red} Figure?}. 

\subsection{Completely random measures (CRMs)}
The previous section assumed a known number of neurons $N$. In practice however, our recordings are a superposition of the outputs of an unknown
number of neurons. We deal with this uncertainty by taking a nonparametric Bayesian approach, and letting the number of neurons $N$ tend to infinity
{\color{red} Can we provide a biogical/electrical justification of this?}. 
Such an approach leads to an elegant and flexible modelling framework, and has  already proved successful in neuroscience applications
\citep{WoodBla2008}.
Since only a finite number of spikes are observed in any finite interval, the total rate $R$ must 
also be finite; moreover, as we described earlier, we want this to be dominated by a few $r_i$. 
A natural framework that captures these  modelling requirements is that of completely random measures \citep{Kingman:PJM67}.
Completely random measures are stochastic processes that form flexible and convenient priors over
infinite dimensional objects like probability distributions \citep{JamesLP09}, hazard functions \citep{Hjo1990}, latent features \citep{ThiJor2007} etc. 
These have been well studied in the Bayesian nonparametrics and machine learning communities, and there exists a wealth of literature on
their theoretical properties, as well as on computational approaches to posterior inference.

Recall that each neuron is characterized by a pair of parameters $(r_i, \theta^*_i)$; the former characterizes the distribution over spike times, 
and the latter over spike
shapes. With equation \eqref{eq:mark_distr} in mind, we map the infinite collection of pairs $\{(r_i, \theta^*_i)\}$ to an atomic measure on $\Theta$:
\begin{align}
  R(\dd \theta) = \sum_{i=1}^{\infty} r_i \delta_{\theta^*_i}
\end{align}
For any subset $\varTheta$ of $\Theta$, the measure $R(\varTheta)$ equals \( \sum_{\{ i: \theta^*_i \in \varTheta \} } r_i\). We allow $R(\cdot)$ to be random,
modelling it as a realization of a completely random measure. Such a random measure has the property that for any two disjoint subsets $\varTheta_1$ 
and $\varTheta_2 \in \Theta$, the measures $R(\varTheta_1)$ and $R(\varTheta_2)$ are independent. 
This distribution over measures is induced by a distribution
over the infinite sequence of weights (the $r_i$'s), and a distribution over the sequence of their locations (the $\theta^*_i$'s). 
For a CRM, the weights $r_i$ are the jumps of a \Levy process \citep{Sato90}, and their distribution is characterized by a 
\Levy measure $\rho(r)$. The locations $\theta^*_i$ are drawn i.i.d.\  from a base probability measure $H(\theta^*)$.
As is typical, we assume these to be independent (though this is not necessary). {\color{red} if there's space, I
can elaborate on the construction of the CRM from its Levy measure, though this is not necessary}

The particular class of CRM is determined by the \Levy measure $\rho(r)$. For our application, we set $\rho(r) = r^{-1}\exp(-r\alpha)$;
this results in a CRM called the Gamma process ($\Gamma$P) \citep{applebaum2004}. 
The Gamma process has the convenient property that the 
total rate $R \equiv R(\Theta) = \sum_{i=1}^{\infty} r_i$ is Gamma distributed (and thus conjugate to the Poisson process prior on $E$).
%\footnote{We abuse notation by using $R$ to denote both the measure as well as the total measure of $\Theta}
%The Gamma distribution has shape parameter $1$ and scale parameter $\alpha$.  Since this is finite almost surely, so too is $E$. 
The Gamma process is also closely connected with the Dirichlet process \citep{Ferguson73}, which will prove useful
later on.
Other choices of the \Levy intensity can be used to capture greater uncertainty in the number of neurons active in any finite interval, or to model
power-law behaviour in the number of spikes emitted by different neurons.

To complete the specification on the Gamma process, we need to choose a base-measure $H(\theta^*)$.
Recalling that $\theta^* \equiv (\mu^*, \Sigma^*)$ gives the mean and variance of the weight-vector $\by$ of each neuron; we set $H(\theta^*)$ 
to be the conjugate normal-Wishart distribution. Our overall model is then:
\begin{align}
  R(\cdot) & \sim \Gamma \text{P}(\alpha, H(\cdot)) \\ %\mathcal{NW}(\mu, \Sigma)) \\
  E_i\ \  &\sim \text{PoissProc}(r_i) \quad i \text{ in } 1,2,\cdots \\
  \tilde{\by}_{ij} & \sim N(\mu^*_i, \Sigma^*_i) \quad i,j \text{ in } 1,2,\cdots \\
  x_i(t) &= \sum_{j = 1}^{|E_i|}  \sum_{k = 1}^{K} \tilde{y}_{ijk} A_k(t - e_{ij}) \\
  X   &= \sum_{i=1}^{\infty} x_i
\end{align}

%Each spike of each neuron is associated with a time $e$ and a weight vector $y$, and one can view the model above as a doubly stochastic Poisson
%process on the product space. 

It will be more convenient to work with the marked Poisson process representation of equations \ref{eq:spk_sup} and \ref{eq:spk_shape}. 
The superposition process $E$ is a rate $R$ Poisson process,
and under a Gamma process prior, $R$ has a Gamma distribution with shape and scale parameters $1$ and $\alpha$ respectively \citep{Ferguson73}.
As we saw (equation \eqref{eq:mark_distr}), the labels $\theta_i$ assigning events to neurons are drawn i.i.d. from a normalized Gamma 
process $G(\dd \theta)$:
\begin{align}
 G(\dd \theta) = \frac{1}{R} \sum_{j=1}^{\infty} r_j
\end{align}
$G(\dd \theta)$ is a random probability measure called a \emph{normalized random measure} \citep{JamesLP09}. Importantly, a 
normalized Gamma process is the Dirichlet process (DP) \citep{Ferguson73}, so that $G$ is a draw from a Dirichlet process. For the $j$ spike in $E$, given its 
parameter $\theta_j$, its shape vector is drawn from a normal distribution
with parameters $(\mu_{j}, \Sigma_{j})$. Thus the spike parameters $\theta$ are i.i.d.\ draws from a Dirichlet process, while the weight vectors are
draws from a DP mixture model of Gaussians (DPMM) \citep{Lo1984}.

The connection with the DP allows us to simplify the representation of our model. In particular, we exploit a remarkable property of the DP that
allows us to integrate out the infinite-dimensional variable $G(\cdot)$. The resulting marginal distribution over observations follows the so-called
 Chinese restaurant process (CRP) \citep{Pit2002a}. Under this scheme, the $j$th spike is assigned the same parameter as an earlier spike with probability 
proportional to the number of earlier spikes having that parameter. It is assigned a new parameter (and thus, a new neuron is observed) with probability 
proportional to $\alpha$. Letting $C_t$ be the number of neurons observed until time $t$, and letting $n_i$ be the number of spikes produced by neuron $i$,
we then have for spike $j$ at time $t = e_j$: 
\begin{align}
  P({\nu_j} = i) & \propto 
  \begin{cases}
   n_i \quad i \in \{1,\cdots, C_{t}\} \\
   \alpha \quad\ i = C_{t} + 1 \\
  \end{cases}  
\label{eq:crp_marg_pr}
\end{align}
%\footnote{Strictly speaking, the process we just described is called a P\'olya urn scheme \citep{BlaMac1973}; for simplicity, we do not distinguish between this and the CRP.}.
and $\theta_j = \theta^*_{\nu_j}$. 
The marginalization property of the DP allows us to integrate out the infinite-dimensional rate vector $R(\cdot)$, and sequentially 
assign spikes to neurons based on the assignments of earlier spikes.
%is assigned (or equivalently, the parameter $\theta$ associated with that neuron). These marks are drawn from a probability measure 
%$G(\dd \theta) = \frac{1}{R} R(\dd \theta)$. From the properties of the Gamma process, the probability measure $G$ a Dirichlet process, 
To do so, we need one final property: for the Gamma process, the random probability measure $G(\cdot)$ is independent of the total mass $R(\Theta)$. 
Consequently, the clustering of spikes (determined by $G(\cdot)$) is independent of the rate $R$ at which they are produced. We then have
 the following model equivalent to the one above:
\begin{align}
  R & \sim \text{Gamma}(1,\alpha) \\
  E &\sim \text{PoissProc}(R) \\
  \theta_j &\sim \text{CRP}(\alpha, H(\cdot)), \quad j = 1,\cdots, |E|   \label{eq:CRP}\\
  \by_j &\sim N(\mu_{j}, \Sigma_{j}), \quad  j = 1,\cdots, |E|   \label{eq:CRP_mix}\\
  x(t) &=   \sum_{j=1}^{|E|} \sum_{k=1}^K y_{jk} A_k(t - e_{j})
\end{align}
Unlike most applications which observe the outputs of a CRP, out observation at any time $t$ a convolution-like function of the CRP outputs of all
earlier times. Consequently, we cannot directly apply standard techniques for posterior inference. In section \ref{sec:inf}, we develop a novel online 
algorithm for posterior inference; first, we provide a discrete-time approximation to our model.

%For neuron $i$, the sequence of spike times is distributed as a Poisson process with random rate $r_i$.
%Each event $e_{ij}$ is associated with a mark or label $y_{ij}$ drawn from a normal distribution (again, with random parameters).
%More broadly, we can view the superposed process $E$ as a rate $R$ Poisson process, with each event having a pair of marks, the neuron identity $i$,
%and weight $y$. From the properties of the Gamma process, the pair form a draw from a Dirichlet process.
%Our data is in a form that makes discrete-time modelling more natural, and an approach now is
%one based on the Beta process-binomial process.

\subsection{A discrete-time approximation}
In the previous subsections, we modelled the continuous-time voltage waveform output by a neuron. Our data on the other hand consists of recordings
at a discrete set of times. While it is possible to make inferences about the continuous-time process that underlies these discrete recordings,
in this paper, for simplicity, we restrict ourselves to the discrete case. We thus provide a discrete-time approximation to the model above. 
Our approximation follows easily from the marked Poisson process characterization of the model.

Recall first the Bernoulli approximation to the Poisson process: a sample from a Poisson process with rate $R$ can be approximated by discretizing
time at a granularity $\Delta$, and assigning each bin an event independently with probability $R\Delta$. The accuracy of this approximation increases 
as $\Delta$ tends to $0$.
%
%This suggests the following approximation at a time resolution $\Delta$. Draw the random Poisson process rate $R$ drawn from a Gamma$(1,\alpha)$ 
%distribution. Simultaneously, draw a random probability measure
% $G$ from a Dirichlet process. Assign an event to an interval independently with probability $R\Delta$, and to each event, assign a random mark drawn 
In our case, we have to approximate the marked Poisson process $E$. All this requires additionally is to assign marks $\theta_i$ and $\by_i$ to each event 
in the Bernoulli approximation. Following equations \eqref{eq:CRP} and \eqref{eq:CRP_mix}, the $\theta_i$'s are distributed according
to a Chinese restaurant process, while each $\by_i$ is drawn from a normal distribution parametrized by the corresponding $\theta_i$. We discretize the 
elements of dictionary $\mathbf{A}$ as well, and each element is now a finite-dimensional vector instead of a function. We set the length of each
dictionary element to $L$, so that $\bA$ is an $K$-by-$L$ matrix. The shape of the $j$th spike is now a vector of length $L$, and for a weight vector
$\by$, is given by $\bA \by$.

\subsection{Correlations in time and across electrodes}
We now describe three extensions to the model outlined previously. The first is the inclusion of measurement noise: 
{\color{red} Biology?}. Let $\epsilon_t$ be the noise at time $t$, we model this as independent, additive and Gaussian.
%However, rather than modelling the noise as independent across time, we model it as a first-order autoregressive process. This can capture
%effects like the movement of electrodes during the experiment. 

Our second extension allows the parameters of each neuron to evolve with time {\color{red} Justify, eg cells dying}. Recall that for neuron $i$, the 
associated parameters are ($\mu^*_i, \Sigma^*_i$). We keep the covariance $\Sigma^*_i$ fixed, however we model the mean $\mu^*_i$ as a first-order
autoregressive process. 

Finally, all we have described so far is the input to a single electrode, our observations on the other hand are multielectrode recordings. 
A simple approach has all electrodes having the same signal (but independent noise); we relax this by only requiring the signals to be
correlated across electrodes {\color{red} Need to check how we set details of this correlation matrix}.
than modelling
